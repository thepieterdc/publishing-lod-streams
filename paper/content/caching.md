## Data caching
{:#caching}

This chapter will list, describe, and compare existing techniques used to cache large open datasets.
Today caching is a viable strategy to work with datasets, because it is convenient to process complex queries locally.
This chapter will not only talk about data caches themselves, but also about multiple techniques to update caches dynamically using both push and pull protocols and about which subset of the data needs to be cached.

### Updating data caches
A common approach for handling large open datasets is caching these datasets locally and executing queries on these caches. In this chapter, we will discuss multiple techniques to update such a cache and we will try to compare them.

#### Pull-based approaches
There are many different approaches for updating large open data caches using pull-based approaches. All of them implement an algorithm that will decide when a piece of data, needs to be updated. These algorithms range from simple well-known algorithms to complex time dependant algorithms. First of all we will compare some basic strategies as described by [Dividino et al.](cite:cites ISI:000374242500024). The first algorithm described is Age: this will update the oldest piece of data first. The second one is Size_SmallestFirst, this will update the smallest piece of data first. The reverse of this: Size_BiggestFirst is also described. Next we have PageRank, this algorithm will select the highest page in your dataset to upload. Then there is ChangeRatio, this will select the most changed data to be updated first. These are the basic algorithms described in this paper, but they also propose some more interesting algorithms. First there is ChangeRate-J en ChangeRate-D that will update the data considering respectively the most changed Jaccard distance or the most changed Dice coefficients on the last two retrieved versions. They also propose the algorithms Dynamics-J and Dynamics-D updating the most dynamic data source based again on the Jaccard distance or the Dice coefficients. This paper concluded that these last 4 algorithms work better when there is limited bandwidth.

Now there are also some more complex algorithms being used. [Nishioka et al.](cite:cites nishioka_scherp_2017) describe how the life-time of RDF Triples can be predicted and how to use these predictions to update local caches. To predict the lifespan of an RDF triple, they have trained a linear regression model on the RDF triples Subject PLD (pay level domain), the predicate and the object form/PLD depending on if the object is a literal or a URI. The predictions made by this model are substantially better than if they just took the mean average. Using this predicted life-time, they propose an algorithm to update a local cache based on the reciprocal of the mean average of a triple. This algorithm was compared with the best performing algorithm proposed by Dividino et al. They concluded that their method outperformed those proposed by Dividino et al. and that once the model was trained, it required fewer resources because they didn't require previous snapshots of the data. They also discussed using more advanced machine learning models but these models didn't have a significant improvement on the results.

A final algorithm being discussed here is application-aware change prioritization together with Preference Aware Source Update proposed by [Akhtar et al.](cite:cites Akhtar2018). This algorithm utilizes a change metric together with a weight function to identify the recent changes. They have compared their algorithm to the already existing solutions described in this chapter and they have concluded that their approach outperforms the other algorithms.

#### Push-based approaches
The previously described approaches all used a pull-based approach, where the local cache will request the data it thinks should be updated. In this chapter we will discuss push-based approaches, where the server notifies caches which part of the data has been changed. This approach has some obvious advantages and disadvantages. The main advantage is that we will limit the amount of requests for data updates while still keeping a complete up to date cache. The main disadvantage is that there will be a bigger load on the server, especially when there exist a lot of caches that the server needs to notify each time a part of the data has changed.

[Rojas Mel√©ndez et al.](cite:cites 10.1145/3184558.3191650) discuss the advantages of a Publish/Subscribe approach (push) and try to prove this with a proof of concept using parking data in Flanders. They concluded that they did not have a large enough dataset to verify their arguments, but they still argue to ease of implementation for push-based strategy

#### Comparing Push and Pull-based approaches
This chapter will compare push and pull-based approaches. These approaches have been discussed in detail in the previous two chapters and it has been shown that both can be used in real-life applications. [Van de Vyvere et al.](cite:cites van2020comparing) compare three metrics for a Server Send Event approach and an HTTP polling approaches (with and without nginx). These metrics were latency, the amount of time it takes for an update to be registered by the user, memory and CPU usage. They concluded that latency for a push-based approach was always lower than the latency on a pull-based approach. Next they concluded that the memory usage of a push-based approach was higher than a pull-based approach, this is because all the connections to the users needed to be kept in memory. They thought that CPU usage would be initially better for a push-based approach, but that pull-based approaches would do better for a higher amount of users, but they had to reject this hypothesis based on their results. This comparison shows that a push-based approach would be generally better. However they didn't use a smart pulling approach for this comparison, as described in chapter (pull-based approaches).


### Caching query results
Until now, we only looked at caching approaches that cached the whole or part of the dataset and then performed queries on this cached dataset. However if the dataset becomes too large, this is impractical. In this chapter we will take a look at caching the results of queries instead.

The first cache strategy that we will take a look at is described [Akhtar et al.](cite:cites ISI:000532866000001). They propose an Adaptive Cache Replacement strategy that will look at query patterns from clients and will prefetch the results of possible future queries. This strategy has been tested and the query times were on average reduced by 6.34% in comparison with existing approaches such as LRU (Least Recently Used), LFU (Least Frequently Used) and SQC (SPARQL Query Caching).

The second cache mechanism that will be considered is a two layer cache proposed by [Yi et al.](cite:cites ISI:000349634200003). As the name suggests, this cache consists of 2 layers. The first layer implements the ARC-memory algorithm (Adaptive Replacement Cache) and will be running in memory. This layer should catch all the frequently used queries that aren't too complicated or for which the result set isn't too large. For these cases we look at the second layer of this mechanism. This layer will be running in a key-value database that will only be partially running in memory. They have compared this method to existing techniques such as LRU and they have shown that their method has a higher hit ratio.

Another methodology for caching Linked Open datasets is using a decentralized cache, this method has been described by [Folz et al.](cite:cites Folz2015CyCLaDEsAD). They propose to build a 'neighborhood' of clients hosting related fragments in their cache, a client will first look at his neighborhood before querying the server directly. This is based on the assumption that clients that have had similar queries in the past, will also have similar queries in the future. They first make a random subset of clients and then select the `k` clients that are the most similar to construct this 'neighborhood'. Each client will use a LRU cache with a fixed size. We can immediately see that it might be useful to combine this decentralized cache system with the two layer cache mechanism described above. They concluded that their decentralized approach catches a lot of queries that just 1 local cache didn't catch and that it would be indeed useful to use this in applications where the load on the main server can be very high.

### Caching in Content Delivery Networks
Caching is an important part of Content Delivery Networks (CDN), they depend on state of the art caching strategies to distribute internet content worldwide.
This chapter will take a look at existing papers about these CDN caching strategies, that we think might be useful in the context of linked open data, and suggest how these can be used.

The first technique is proposed by [Berger](cite:cites 10.1145/3286062.3286082) and concerns a supervised learning technique to train a lightweight boosted decision tree, using calculated optimal decisions from the past. They will use this model to update the cache in the present. This is called Learning From OPT (LFO). They state that this model achieves a 93% accuracy in predicting the OPT decisions, but their is a 20% gap in caching performance between OPT en LFO. So even tough the model predicts the absolute best decision in 93% of the cases, it's caching performance is significantly lower that the most optimal model. We feel that this method could also be applied as a caching strategy for query results caches of linked open data, but it still needs to be compared to other strategies described in the previous chapter.

Current cache replacement algorithms mostly consider the frequency and locality of data. [Li et al.](cite:cites 10.1109/ICPADS.2012.106) propose to use the access interval change rate as a better metric to update caches. They implemented a naive algorithm that uses this access interval change rate and compared it to basic algorithms such as LRU and LFU. It showed that using the access interval change rate, resulted in a better cache performance. This method is fairly simple, yet still powerful. In the chapter about pull-based strategies we saw that change rate methods also outperformed these basic algorithms, we feel that these algorithms might be of good use, if we want to implement a simple strategy.

### HTTP Caching

Caching is used a lot on the world wide web, mostly in the form of HTTP caching. In this chapter we will discuss how HTTP caching works and compare it to other caching strategies discussed in previous paragraphs.
HTTP caching is mostly used for caching results of an HTTP GET request. The parameters of caching are set in the headers of the replies.  We can give the cache-control header multiple values:

1. No-store: do not cache this.
2. No-cache: Cache this result, but every time we need it, validate it by requesting the header again from the server
3. Private: Only cache this in a private browser cache and not intermediate servers
4. Public: This item can be cached everywhere
5. Max-age=<seconds>: This will tell. The cache how long it can keep this item in cache without validating it
6. Must-revalidate: instructs the cache to revalidate the content when it would be reused.

If this header is not defined, the cache may use a heuristic that is 10% of the difference in time between the current timestamp and the timestamp in the last-modified header. We can compare this to previous discussed dynamic based algorithms.
If a cached document has expired, there are 2 things that can happen: the document will be validated or fetched again.

We see that all the different algorithms are either based on age or on dynamics. In the paragraph about pull-based algorithms we discussed these kinds of algorithms and we noticed that other approaches had a better performance. This is why we propose that in the context of large open data, these other approaches discussed in previous chapters could have a better performance.
