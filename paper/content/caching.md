## Data caching
{:#caching}

This chapter will list, describe, and compare existing techniques used to cache large open datasets.
Today caching is a viable strategy to work with datasets, because it is convenient to process complex queries locally.
This chapter will not only talk about data caches themselves, but also about multiple techniques to update caches dynamically using both push and pull protocols and inform the client about which subset of the data needs to be cached.

### Updating data caches
A common approach for handling large open datasets is caching these datasets locally and executing queries on these caches. In this chapter, we will discuss multiple techniques to update such a cache and we will compare them.

#### Pull-based approaches
There are many different approaches for updating large open data caches using pull-based approaches. All of them implement an algorithm that will decide when a piece of data needs to be updated. These algorithms range from simple well-known algorithms to complex time dependent algorithms. First of all, we will compare some basic strategies as described by [Dividino et al.](cite:cites ISI:000374242500024). The first algorithm described is *Age*, in which the oldest piece of data is updated first. The second one is *Size_SmallestFirst*, this will update the smallest piece of data first. The reverse of this: *Size_BiggestFirst* is also described. Next we have *PageRank*, this algorithm will select the highest page in your dataset to upload. Then there is *ChangeRatio*, this will select the most changed data to be updated first. These are the basic algorithms described in this paper, but they also propose some more interesting algorithms. First there is *ChangeRate-J* and *ChangeRate-D* that will update the data considering the most changed Jaccard distance or the most changed Dice coefficients respectively, taking into account the last two retrieved versions. They also propose the algorithms *Dynamics-J* and *Dynamics-D*,that update the most dynamic data source, again based on the Jaccard distance or the Dice coefficients. They conclude that these final four algorithms work better when there is limited bandwidth.

Other more complex algorithms are being used as well. [Nishioka et al.](cite:cites nishioka_scherp_2017) describe how the life-time of RDF Triples can be predicted and how to use these predictions to update local caches. To predict the lifespan of an RDF triple, they have trained a linear regression model on the RDF triples Subject pay level domain (PLD), the predicate and the object form/PLD depending on if the object is a literal or a URI. The predictions made by this model are substantially better than if they just apply the mean average of frequencies in the training data to all triples in the test set. Using this predicted life-time, they propose an algorithm to update a local cache based on the reciprocal of the mean average of a triple. This algorithm was compared with the best performing algorithm proposed by Dividino et al., specifically the *Dynamics-J* algorithm. This comparison was done on the DYLDO dataset with a varying amount of bandwidth. For every bandwidth, the linear regression model outperformed the *Dynamics-J* algorithm for every bandwidth, but the difference was largest for a bandwidth of 10-20% of the size of the dataset. Here the precision for the linear model was 0.998, while the precision for the *Dynamics-J* algorithm was only 0.997.
They concluded that their method outperformed those proposed by Dividino et al. and that once the model was trained, it required fewer resources because they didn't require previous snapshots of the data. They also discussed using more advanced machine learning models but these models didn't have a significant improvement on the results.

A final algorithm being discussed here is application-aware change prioritization together with Preference Aware Source Update proposed by [Akhtar et al.](cite:cites Akhtar2018). This algorithm utilizes a change metric together with a weight function to identify the recent changes. They have compared their algorithm to the already existing solutions described in this chapter. This comparison has been done on the DYLDO and BTC datasets. First they compared the effectivity of their proposed algorithm with the *Pagerank, Size, Age, Changeratio* and *Changerate* algorithms. Their algorithm outperformed all of these algorithms with an effectivity of 93.5%, which is 8.8% higher than the *Changerate* algorithm, who came in second. They have also made a seperate comparison between their algorithm and the linear regression model, proposed in the previous section. This has been done in an iterative setup and the algorithm proposed by this paper had a higher precision and recall than the linear model for every iteration. With the largest difference at the first iteration which was a difference of 0.03 for both precision and recall on the BTC dataset. They concluded that their algorithm outperformed the current state of the art.

#### Push-based approaches
The previously described approaches all used a pull-based mechanism, where the local cache will request the data it thinks should be updated. In this section we will discuss push-based approaches, where the server notifies caches which part of the data has been changed. This approach has some obvious advantages and disadvantages. The main advantage is that the technique will limit the amount of requests for data updates while still keeping a complete up to date cache. The main disadvantage is that there will be a bigger load on the server, especially when there exist a lot of caches that the server needs to notify each time a part of the data has changed.

[Rojas Mel√©ndez et al.](cite:cites 10.1145/3184558.3191650) discuss the advantages of a Publish-Subscribe approach and use parking data in Flanders as a proof of concept. They concluded that they did not have a large enough dataset to verify their arguments, but they still argue the ease of implementation for a push-based strategy

#### Comparing Push and Pull-based approaches
This section will compare push and pull-based approaches, that have been discussed in previous subsections. [Van de Vyvere et al.](cite:cites van2020comparing) compare three metrics for an SSE approach and HTTP polling approaches (with and without nginx). These metrics were latency, the amount of time it takes for an update to be registered by the user, memory and CPU usage. They concluded that latency for a push-based approach was always lower than the latency on a pull-based approach. While the memory usage of a push-based approach was higher than a pull-based approach, they argue that this is because all the connections to the users needed to be kept in memory. The author assumed that CPU usage would be initially better for a push-based approach, but that pull-based approaches would do better for a higher amount of users, but they had to reject this hypothesis based on their results. Overall, their comparison shows that a push-based approach would generally be better. However caution should be taken, as the authors did not use a smart pulling approach for this comparison, as described in the section about pull-based approaches.


### Caching query results
Until now, we only looked at caching approaches that cached the whole or part of the dataset and then performed queries on this cached dataset. However if the dataset becomes too large, this is impractical. In this chapter we will take a look at caching the results of queries instead.

The first cache strategy that we will take a look at is described [Akhtar et al.](cite:cites ISI:000532866000001). They propose an adaptive cache replacement strategy that looks at query patterns from clients and prefetches the results of possible future queries. This study shows that the query times were on average reduced by 6.34% in comparison with existing approaches such as LRU (Least Recently Used), LFU (Least Frequently Used) and SQC (SPARQL Query Caching).

The second cache mechanism that is considered is a two layer cache proposed by [Yi et al.](cite:cites ISI:000349634200003). As the name suggests, this cache consists of two layers. The first layer implements the Adaptive Replacement Cache-memory algorithm (ARC) and will be running in memory (RAM). This layer should catch all the frequently used queries that are not too complicated or for which the result set is not too large. For these cases the second layer of this mechanism is used. This layer runs in a key-value database that is only partially running in memory. They have compared this method to existing techniques such as LRU and they showed that their method has a higher hit ratio.

Another methodology for caching Linked Open datasets is using a decentralized cache, such as described by [Folz et al.](cite:cites Folz2015CyCLaDEsAD). They propose to build a 'neighborhood' of clients hosting related fragments in their cache, a client will first look at his neighborhood before querying the server directly. This is based on the assumption that clients that have had similar queries in the past, will also have similar queries in the future. The algorithm first makes a random subset of clients and then selects the `k` clients that are the most similar, to construct this 'neighborhood'. Each client will use a LRU cache with a fixed size. It can be seen that it might be useful to combine this decentralized cache system with the two layer cache mechanism described above. The authors concluded that their decentralized approach catches a lot of queries that that a single local cache would not be able to catch. They propose that their method would indeed be useful to use in applications where the load on the main server can be very high.

### Caching in Content Delivery Networks
Caching is an important part of Content Delivery Networks (CDN), they depend on state-of-the-art caching strategies to distribute web content worldwide.
This chapter takes a look at existing papers about CDN caching strategies, that we think might be useful in the context of linked open data, and suggest how these can be used.

<!-- Not "Berget et al.", it's one person -->
The first technique is proposed by [Berger](cite:cites 10.1145/3286062.3286082) and concerns a supervised learning technique to train a lightweight boosted decision tree, using calculated optimal decisions (OPT) from the past. They will use this model to update the cache in the present. This is called Learning From OPT (LFO). They state that this model achieves a 93% accuracy in predicting the OPT decisions, but there is a 20% gap in caching performance between OPT and LFO. So even though the model predicts the absolute best decision in 93% of the cases, its caching performance is significantly lower that the most optimal model. We argue that this method could also be applied as a caching strategy for query results caches of linked open data, but it still needs to be compared to other strategies described in the previous sections.

Current cache replacement algorithms mostly consider the frequency and locality of data. [Li et al.](cite:cites 10.1109/ICPADS.2012.106) propose to use the access interval change rate as a better metric to update caches. They implemented a naive algorithm that uses this access interval change rate and compared it to basic algorithms such as LRU and LFU. They show that using the access interval change rate resulted in a better cache performance. This method is fairly simple, yet still powerful. In the section about pull-based strategies we saw that change rate methods also outperformed these basic algorithms, we argue that these algorithms might be of good use, if we want to implement a simple strategy.

### HTTP Caching

Caching is used a lot on the World Wide Web, mostly in the form of HTTP caching. In this chapter we will discuss how HTTP caching works and compare it to other caching strategies discussed in previous paragraphs.
HTTP caching is mostly used for caching results of an HTTP GET request. The caching parameters are set in the headers of the replies. The replier can give the cache-control header multiple values:

1. No-store: Do not cache this.
2. No-cache: Cache this result, but every time it is needed, validate it by requesting the header again from the server.
3. Private: Only cache this in a private browser cache and not on intermediate servers.
4. Public: This item can be cached everywhere
5. Max-age=<seconds>: This will tell the cache how long it can keep this item in cache without validating it.
6. Must-revalidate: Instructs the cache to revalidate the content when it would be reused.

If this header is not defined, the cache may use a heuristic that invalidates the cache when 10% of the difference in time between the current timestamp and the timestamp in the last-modified header has passed. This can be compared to previous discussed dynamic based algorithms.
If a cached document has expired, there are 2 things that can happen: the document will be validated or be fetched again.

It can be noticed that all the different algorithms are either based on age or on dynamics. In the paragraph about pull-based algorithms we discussed these kinds of algorithms and we noticed that other approaches had a better performance. This is why we propose that in the context of large open data, these other approaches discussed in previous sections could have a better performance.
