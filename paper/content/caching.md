## Data caching
{:#caching}

This section describes and compares various techniques in literature that can be used to enable caching of large open datasets. Multiple aspects of caching are considered, the first is an overview of pull- and push-based strategies to update caches, and to notify consumers of changes. Next, techniques to cache query results are discussed. This section is concluded by providing a case study of caching on the Web, more specifically in Content Delivery Networks.

### Pull-based approaches
{:#caching-pullbased}

Many different algorithms exist to update large Open Data caches using pull-based approaches, with varying computational complexity. [Dividino et al.](cite:cites ISI:000374242500024) consider the following five elementary strategies: 

1. **Age:** First update the oldest part of the dataset.
2. **Size_SmallestFirst:** First update the smallest part of the dataset.
3. **Size_BiggestFirst:** The opposite of the previous strategy, first update the biggest part of the dataset.
4. **PageRank:** Update the highest-ranked page in the dataset.
5. **ChangeRatio:** Update the part of the dataset that was changed the most (using set difference).

In addition to these simple algorithms, [Dividino et al.](cite:cites ISI:000374242500024) propose two more complex techniques. These techniques each have two variants, one using the Jaccard distance (ending in `-J`) and another one with the `-D` suffix which uses the Dice coefficient, taking into account the last two retrieved versions. The first algorithm is *ChangeRate*. This algorithm is similar to the simple *ChangeRatio* algorithm, but uses the Jaccard distance or Dice coefficient instead of set difference. Finally, the authors propose the *Dynamics-J* and *Dynamics-D* algorithms, that update the most dynamic data source. The authors conclude that these final four algorithms work better when there is limited bandwidth.

[Nishioka et al.](cite:cites nishioka_scherp_2017) describe how the lifespan of RDF Triples can be predicted and how these predictions can be used to update local caches. These predictions are obtained by training a linear regression model on different features of the RDF triple. The first feature is the *Pay-level domain (PLD)* of the subject. For example, the PLD of the subject ``https://dbpedia.org/resource/Google`` would be ``https://dbpedia.org`. As a motivation for this choice, the authors state that triples from the same PLD will have a similar lifespan. Additionally, the predicate and the Pay-level domain of the object are used. If the object is a literal, rather than a URI, the literal value is used instead. When compared to taking the mean average of the frequencies in the training data to all triples in the test set, the predictions provided substantially better results. Using this predicted lifespan, the authors propose an algorithm to update a local cache based on the reciprocal of the mean average of a triple. This algorithm was compared with the best performing algorithm proposed by [Dividino et al.](cite:cites ISI:000374242500024), which is the *Dynamics-J* algorithm. This comparison was done on the DyLDO and Wikidata datasets with a varying amount of bandwidth. For every bandwidth, both algorithms offer very similar precision, but the precision of the linear regression model is always higher than the *Dynamics-J* algorithm. This difference is the most significant when the bandwidth is low, in which the precision for the linear model is 0.998, whereas the Dynamics-J algorithm has a precision of 0.996. As the relative bandwidth increases, both algorithms achieve a precision of 100%. The authors conclude that their approach outperformed the methods proposed by [Dividino et al.](cite:cites ISI:000374242500024), and that it requires fewer resources because they previous snapshots of the data are not required. Finally, the authors discussed more advanced machine learning models, which did not incur a significant improvement on the results.

The final pull-based algorithm is *Application-Aware Change Prioritization (AACP)*, using the *Preference Aware Source Update (PASU)* strategy. This algorithm proposed by [Akhtar et al.](cite:cites Akhtar2018) combines a change metric and a weight function to identify the recent changes. The authors have compared their algorithm to the already existing solutions described in this section. This comparison has been done on the DyLDO and BTC datasets. When compared to the five elementary algorithms and the *ChangeRate* algorithm by [Dividino et al.](cite:cites ISI:000374242500024), the authors found that their algorithm outperforms every algorithm with an effectivity of 93.5%, which is 8.8% higher than the second-ranked *ChangeRate* algorithm. Their algorithm was also compared to the aforementioned linear regression model by [Nishioka et al.](cite:cites nishioka_scherp_2017), in which their algorithm has higher precision and a recall score than the linear model, for every iteration. This difference is the most significant during the first iteration, namely 0.03 for both precision and recall on the BTC dataset.

### Push-based approaches
The approaches described in the previous section rely on pull-based mechanisms. Using these mechanisms, the client is responsible for updating their local cache themselves. Push-based approaches work differently, in the sense that the server notifies clients of changes in the dataset. The main advantage of this strategy is that the amount of unnecessary requests is reduced since clients do not need to poll for updates to the dataset. However, pushing updates to the clients will temporarily result in a bigger load on the server.

[Rojas Mel√©ndez et al.](cite:cites 10.1145/3184558.3191650) discuss the advantages of a *publish-subscribe* approach and use parking data in Flanders as a proof of concept. While the authors did not have a large enough dataset to verify their arguments, they still argue the ease of implementation of a push-based strategy.

Finally, [Van de Vyvere et al.](cite:cites van2020comparing) compare three metrics for an SSE approach and HTTP polling approaches (with and without nginx[^nginx]). These metrics are latency (the amount of time it takes for an update to be registered by the consumer), memory usage, and CPU usage. The authors conclude that pull-based approaches always incur a higher latency than push-based approaches. On the other hand, more memory is required to use push-based approaches, because all connections to clients need to be kept alive in memory. The authors assumed that CPU usage would be better for a push-based approach initially, but that for a higher amount of consumers, the pull-based approaches would perform better. However, this hypothesis was rejected based on the results. Overall, their comparison shows that a push-based approach is generally the best choice. It is important to remark that the authors have used a a simplistic pulling approach for this comparison, as opposed to the more complex approaches described in the previous section.

[^nginx]: A high-performant web server and reverse proxy (http://nginx.org)

### Caching query results
The aforementioned approaches always assume that the entire dataset is stored at every consumer. However, if the dataset becomes too large, this is not a viable option. Alternatively, the results of queries can be cached instead. This section describes three approaches to achieve this.

The first cache technique is proposed by [Akhtar et al.](cite:cites ISI:000532866000001). This technique uses an adaptive cache replacement strategy that examines query patterns from clients, and prefetches the results of possible future queries accordingly. This study shows an average reduction of query times by 6.34% when compared to existing approaches such as LRU (Least Recently Used), LFU (Least Frequently Used) and SQC (SPARQL Query Caching).

Secondly, [Yi et al.](cite:cites ISI:000349634200003) propose a two-layer caching mechanism. The first layer implements the Adaptive Replacement Cache-memory algorithm (ARC) and operates in memory. The purpose of this layer is to cache all the frequently used queries that are either not complicated, or for which the result set is small. The other cases are handled by the second layer. This layer is a key-value database that is only partially running in memory. The approach has been compared to existing techniques on the OLTP-trace test set. The results prove that the proposed method has a higher hit ratio, more specifically for a cache size of 1000, an LRU approach has a 0.43134 hit rate, while the ARC algorithm achieves a 0.44938 hit rate.

The final method for caching Linked Open datasets is the CyCLaDEs approach, proposed by [Folz et al.](cite:cites Folz2015CyCLaDEsAD). This approach consists of a decentralized cache that uses a ''neighborhood'' of clients. Every client in the same neighborhood hosts related fragments in their cache. Subsequently, a new client will first contact clients in this neighborhood before querying the server. This idea assumes that clients that have had similar queries in the past will also perform similar queries in the future. The algorithm first constructs a random subset of clients and then selects the `k` clients that are the most similar, to compose a neighborhood. Each client uses an LRU cache with a fixed size. It might be useful to combine this decentralized cache system with the two layer cache mechanism described above. The authors conclude that their decentralized approach is able to handle a lot of queries that a single local cache would miss. Furthermore, the CyCLaDEs approach with 10 neighbors was compared to existing caching approaches on the BSBM-1M dataset. In both cases, the local caches can handle 40% of the total requests, but in the CyCLaDEs approach, 22% of the requests could be served from the neighborhood. They propose that their method could indeed be used in applications where the load on the main server can be very high. The authors did not explicitly investigate this, but this can be compared in future work.

### HTTP Caching
Caching is used a lot on the Web, mostly in the form of HTTP caching. Given the many different HTTP methods that exist, it only makes sense to cache *safe* requests that do not modify the state of the server, since these cannot invalidate the data. The most prominent examples of these requests are `HEAD`, `GET`, and `OPTIONS` requests, of which `GET` requests are the most common. In order to indicate how a request can be cached, the server can include a `Cache-Control` header in the response. This response has several possible values, multiple can be combined.

1. **no-store:** This response may not be cached by the client.
2. **no-cache:** This response may be cached by the client, but everytime it is requested, the client must send a `HEAD` request to the server to validate whether the cached result is still up to date.
3. **private:** This response may only be cached in a web browser and not on intermediate servers. This option can for example be used when the data is confidential.
4. **public:** Every server (including intermediaries) may cache this response.
5. **max-age=x:** The response may be cached for at most `x` amount of seconds, without revalidating.
6. **must-revalidate:** The response must be revalidated everytime it is requested.

In addition to the `Cache-Control` header, the server may include an *etag* in the response that is calculated as a hash of the content. When sending this request again in the future, the client can include an `If-Modified` header with this etag as its value. If the server detects that the etag is still valid for the current version of the dataset, it sends a `HTTP 304 Not Modified` status code, indicating that the cached copy of the client is up to date.

If no caching headers are present in the response, the client may use a heuristic that evicts the response from its cache when 10% of the difference in time between the current timestamp and the timestamp in the `Last-Modified` header is elapsed. This can be compared to previous discussed dynamic based algorithms. If a cached document is expired, the client can either send the `GET` request again and cache the new response, or send a lightweight `HEAD` request to validate the state.

Note that HTTP caching only uses age or dynamics, which corresponds to the pull-based approaches described in [](#caching-pullbased). It was argued that push-based approaches had a better performance, which is why we propose that further research is required towards applying push-based approaches in HTTP communication.

### Caching in Content Delivery Networks
An example of an intermediary in HTTP caching is a Content Delivery Networks (CDN), which distribute cached copies of webpages over multiple servers that are located around the world. As a result, the perceived performance of a website no longer depends on the distance of the client to the server. Since these networks rely heavily on state of the art caching strategies, it is worth investigating scientific literature that considers Content Delivery Networks and try to apply the findings to Linked Open Data.

<!-- Not "Berget et al.", it's one person -->
The first technique is proposed by [Berger](cite:cites 10.1145/3286062.3286082) and concerns a supervised learning technique to train a lightweight boosted decision tree, using calculated *optimal decisions (OPT)* from the past. This model is subsequently used to update the cache in the present, a technique called *Learning from OPT (LFO)*. The authors state that this model achieves an accuracy of 93% in predicting the OPT decisions, but there is a 20% gap in caching performance between OPT and LFO. So even though the model predicts the best decision in 93% of the cases, its caching performance is significantly lower that the most optimal model. We argue that this method could also be applied as a caching strategy for query results caches of linked open data, but it still needs to be compared to other strategies described in the previous sections.

Current cache replacement algorithms mostly consider the frequency and locality of data. [Li et al.](cite:cites 10.1109/ICPADS.2012.106) propose to use the access interval change rate as an alternative metric to update caches. In this work, a naive algorithm is implemented that uses this access interval change rate and compares it to basic algorithms such as LRU and LFU. The authors show that using the access interval change rate results in a better caching performance. While this method is fairly simple, it is very performant. Combined with the conclusions from [](#caching-pullbased), we therefore conclude that these algorithms are beneficial if a simple strategy is needed, but that even better performance can be achieved with more complex algorithms.
