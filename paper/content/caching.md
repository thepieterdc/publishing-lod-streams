## Data caching
{:#caching}

This section lists, describes, and compares existing techniques used to cache large open datasets.
It does not only talk about data caches themselves, but also about techniques to update caches dynamically using both push and pull protocols and inform the client about which subset of the data needs to be cached.

### Pull-based approaches
There are many different approaches for updating large open data caches using pull-based approaches. These algorithms range from simple algorithms to complex time dependent algorithms. First of all, some basic strategies are compare by [Dividino et al.](cite:cites ISI:000374242500024). The first algorithm described is *Age*, in which the oldest piece of data is updated first. The second one is *Size_SmallestFirst*, which updates the smallest piece of data first. The reverse of this, *Size_BiggestFirst*, is also described. Next there is *PageRank*, which selects the highest-ranked page in your dataset to update. Then there is *ChangeRatio*, which selects the most changed data to be updated first. These are the basic algorithms described in this paper, but they also propose some more interesting algorithms. First, there is *ChangeRate-J* and *ChangeRate-D* that updates the data considering the most changed Jaccard distance or the most changed Dice coefficients respectively, taking into account the last two retrieved versions. They also propose the algorithms *Dynamics-J* and *Dynamics-D*, that update the most dynamic data source, again based on the Jaccard distance or the Dice coefficients. They conclude that these final four algorithms work better when there is limited bandwidth.

Other more complex algorithms are being used as well. [Nishioka et al.](cite:cites nishioka_scherp_2017) describe how the life-time of RDF Triples can be predicted and how to use these predictions to update local caches. To predict the lifespan of an RDF triple, they trained a linear regression model on the RDF triples Subject pay level domain (PLD), the predicate and the object form/PLD depending on if the object is a literal or a URI. The predictions made by this model are substantially better than if they just apply the mean average of frequencies in the training data to all triples in the test set. Using this predicted life-time, they propose an algorithm to update a local cache based on the reciprocal of the mean average of a triple. This algorithm was compared with the best performing algorithm proposed by Dividino et al., specifically the *Dynamics-J* algorithm. This comparison was done on the DYLDO dataset with a varying amount of bandwidth. For every bandwidth, the linear regression model outperformed the *Dynamics-J* algorithm, but the difference was largest for a relative bandwidth of 10-20%. Here the precision for the linear model was 0.998, while the precision for the *Dynamics-J* algorithm was only 0.997.
They concluded that their method outperformed those proposed by Dividino et al. and that once the model was trained, it required fewer resources because they didn't require previous snapshots of the data. They also discussed using more advanced machine learning models but these models didn't have a significant improvement on the results.

A final algorithm being discussed here is application-aware change prioritization together with Preference Aware Source Update proposed by [Akhtar et al.](cite:cites Akhtar2018). This algorithm utilizes a change metric together with a weight function to identify the recent changes. They have compared their algorithm to the already existing solutions described in this section. This comparison has been done on the DYLDO and BTC datasets. First they compared the effectivity of their proposed algorithm with the *Pagerank, Size, Age, Changeratio* and *Changerate* algorithms. Their algorithm outperformed all of these algorithms with an effectivity of 93.5%, which is 8.8% higher than the *Changerate* algorithm, which came in second. They have also made a separate comparison between their algorithm and the linear regression model, proposed in the previous section. This has been done in an iterative setup and the algorithm proposed by this paper had a higher precision and recall than the linear model for every iteration. With the largest difference at the first iteration which was a difference of 0.03 for both precision and recall on the BTC dataset. They concluded that their algorithm outperformed the current state of the art.

#### Push-based approaches
The previously described approaches all used a pull-based mechanism, where the local cache requests the data it thinks should be updated. This section discusses push-based approaches, where the server notifies caches which part of the data has been changed. This approach has some obvious advantages and disadvantages. The main advantage is that the technique limits the amount of requests for data updates while still keeping a complete up to date cache. The main disadvantage is that there is a bigger load on the server, especially when there are a lot of caches that the server needs to notify each time a part of the data has changed.

[Rojas Mel√©ndez et al.](cite:cites 10.1145/3184558.3191650) discuss the advantages of a Publish-Subscribe approach and use parking data in Flanders as a proof of concept. They concluded that they did not have a large enough dataset to verify their arguments, but they still argue the ease of implementation for a push-based strategy.

This paragraph compares push and pull-based approaches, that have been discussed in previous paragraphs. [Van de Vyvere et al.](cite:cites van2020comparing) compare three metrics for an SSE approach and HTTP polling approaches (with and without nginx). These metrics were latency, the amount of time it takes for an update to be registered by the user, memory and CPU usage. They concluded that latency for a push-based approach was always lower than the latency on a pull-based approach. While the memory usage of a push-based approach was higher than a pull-based approach, they argue that this is because all the connections to the users needed to be kept in memory. The author assumed that CPU usage would be initially better for a push-based approach, but that pull-based approaches would do better for a higher amount of users, but they had to reject this hypothesis based on their results. Overall, their comparison shows that a push-based approach would generally be better. However caution should be taken, as the authors used a simplistic pulling approach for this comparison, unlike those described in the previous section.

### Caching query results
Until now, only caching approaches that cached the whole or part of the dataset and then performed queries on this cached dataset were discussed. However if the dataset becomes too large, this is impractical. In this section caching mechanisms that cache the result of queries are discussed.

The first cache strategy is described by [Akhtar et al.](cite:cites ISI:000532866000001). They propose an adaptive cache replacement strategy that looks at query patterns from clients and prefetches the results of possible future queries. This study shows that the query times were on average reduced by 6.34% in comparison with existing approaches such as LRU (Least Recently Used), LFU (Least Frequently Used) and SQC (SPARQL Query Caching).

The second cache mechanism that is considered is a two layer cache proposed by [Yi et al.](cite:cites ISI:000349634200003). The first layer implements the Adaptive Replacement Cache-memory algorithm (ARC) and runs in memory. This layer should catch all the frequently used queries that are not too complicated or for which the result set is not too large. For the other cases the second layer of this mechanism is used. This layer runs in a key-value database that is only partially running in memory. They have compared this method to existing techniques such as LRU and they showed that their method has a higher hit ratio, more specifically for a cache size of 1000 LRU has a 0.43134 hit rate, while the ARC algorithm has a 0.44938 hit rate. this comparison was done on the OLTP-trace test set.

Another method for caching Linked Open datasets is to use a decentralized cache, such as described by [Folz et al.](cite:cites Folz2015CyCLaDEsAD). They propose to build a 'neighbourhood' of clients hosting related fragments in their cache, a client will first look at his neighbourhood before querying the server directly. This is based on the assumption that clients that have had similar queries in the past will also perform similar queries in the future. The algorithm first makes a random subset of clients and then selects the `k` clients that are the most similar, to construct this 'neighbourhood'. Each client uses an LRU cache with a fixed size. It can be seen that it might be useful to combine this decentralized cache system with the two layer cache mechanism described above. The authors concluded that their decentralized approach catches a lot of queries that a single local cache would not be able to catch. They compared a normal caching approach with a cyclades approach with 10 neighbours on the BSBM-1M dataset. In both cases, local caches cache 40% of the total requests, but in the cyclades approach 22% percent of requests are also cached in the neighbourhood. They propose that their method would indeed be useful to use in applications where the load on the main server can be very high. They did not explicitly compare their approach for different server loads however, this could be compared in future work.

### Caching in Content Delivery Networks
Caching is an important part of Content Delivery Networks (CDN), they depend on state-of-the-art caching strategies to distribute web content worldwide.
This section takes a look at existing papers about CDN caching strategies, that we think might be useful in the context of linked open data, and suggest how these can be used.

<!-- Not "Berget et al.", it's one person -->
The first technique is proposed by [Berger](cite:cites 10.1145/3286062.3286082) and concerns a supervised learning technique to train a lightweight boosted decision tree, using calculated optimal decisions (OPT) from the past. They use this model to update the cache in the present. This is called Learning From OPT (LFO). They state that this model achieves a 93% accuracy in predicting the OPT decisions, but there is a 20% gap in caching performance between OPT and LFO. So even though the model predicts the absolute best decision in 93% of the cases, its caching performance is significantly lower that the most optimal model. We argue that this method could also be applied as a caching strategy for query results caches of linked open data, but it still needs to be compared to other strategies described in the previous sections.

Current cache replacement algorithms mostly consider the frequency and locality of data. [Li et al.](cite:cites 10.1109/ICPADS.2012.106) propose to use the access interval change rate as a better metric to update caches. They implemented a naive algorithm that uses this access interval change rate and compared it to basic algorithms such as LRU and LFU. They show that using the access interval change rate resulted in a better cache performance. This method is fairly simple, yet still powerful. In the section about pull-based strategies it was discussed that change rate methods also outperform these basic algorithms. We argue that these algorithms might be of good use, if a simple strategy is needed, but there are more complicated algorithms described in this paper, that have a better performance.

### HTTP Caching

Caching is used a lot on the Web, mostly in the form of HTTP caching. This section discusses how HTTP caching works and compares it to other caching strategies discussed in previous paragraphs.
HTTP caching is mostly used for caching results of an HTTP GET request. The caching parameters are set in the headers of the replies. The replier can give the cache-control header multiple values:

1. No-store: Do not cache this.
2. No-cache: Cache this result, but every time it is needed, validate it by requesting the header again from the server.
3. Private: Only cache this in a private browser cache and not on intermediate servers.
4. Public: This item can be cached everywhere
5. Max-age=<seconds>: This tells the cache how long it can keep this item in cache without validating it.
6. Must-revalidate: Instructs the cache to revalidate the content when it would be reused.

If this header is not defined, the cache may use a heuristic that invalidates the cache when 10% of the difference in time between the current timestamp and the timestamp in the last-modified header has passed. This can be compared to previous discussed dynamic based algorithms.
If a cached document has expired, there are 2 things that can happen: the document is validated, here the browser fetches the header of the page and compares it to the header of the cached page, or it is fetched again.

Note that all the different algorithms are either based on age or on dynamics. The paragraph about pull-based algorithms discussed these kinds of algorithms and it was argued that other approaches had a better performance. This is why we propose that in the context of large open data, these other approaches discussed in previous sections could have a better performance.
