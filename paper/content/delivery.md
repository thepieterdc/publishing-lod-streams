## Data delivery
{:#delivery}

In this chapter, we will discuss the transmission of the data to the users. Since we have previously discussed how data can be stored, the next problem that arises is how to expose it for consumers to access. The current most commonly used protocol for web data exchange is HTTP, which uses a `request-response` communication pattern. However, this might not be the most optimal form of communication for our purpose. The MQTT protocol, which uses a `publish-subscribe` pattern or existing software such as Apache Kafka, which employs a similar strategy and uses an own protocol directly over TCP, might be more suitable. Alternatively, regular HTTP can still be viable, however, a new kind of 'protocol' will have to be built on top of it, to better meet the needs of event-based processing.

### Considerations

The partitioning of the data will impact the transmission of it. Since we are working with event streams, our architecture will be decentralized ([](cite:cites delva2020geospatial) also mentions storage and performance benefits, in comparison with a centralized, data-dump style approach).

In the current state of the art, the nature of the data impacts the transmission. In the case of real-time sensor data, for example, [](cite:cites atmoko2017) shows an MQTT-based approach and indicates that this is more efficient than regular HTTP. However, since the goal is the achieve a generic approach for each form of data, elements of different technologies will need to be combined.

### Delivery Approaches

#### Low level protocols

Various low-level protocols exist such as MQTT, AMQP, or CoAP. These protocols are focused on the transmission of data on constrained, low-level devices.
[](cite:cites iotwang) shows a similar conclusion when comparing MQTT and HTTP on data from IoT devices, some datasets in [](cite:cites van2020comparing) will also certainly use IoT data. While the intention is not to use a lightweight, lower level protocol such as MQTT (instead the goal is to publish data on the web, using HTTP), it is valuable to look at what makes it the best option, i.e. the different communication model, and adapt the desired technology so it incorporates this.

Furthermore, low-level protocols like MQTT suffer from security vulnerabilities [](cite:cites cve), which is problematic in the context of open data publishing. HTTP, on the other hand, is extremely established and mainstream and way less susceptible to the same type of problems, hence choosing HTTP is a necessity in terms of security.

#### HTTP-based approaches
Other options include the utilization of **web feeds**, such as (most commonly) RSS. RSS [](cite:cites rsspilgrim) publishes updates in a feed and allows users to access them in a standardized format. However, RSS is an umbrella term that spans different formats. Therefore, Atom ([](cite:cites gregorio2005atom) and [](cite:cites ruby2008rss)) was created, with the goal of achieving more standardization and disambiguation. It uses a separate protocol on top of HTTP. These two approaches to data publishing (which are very similar) should both be considered, since the concept and use of web feeds is similar to what the event-based approach wants to achieve, however, it is is hard to determine a clear ``best option” without doing research (i.e. experimenting with these techniques).


**Linked Data Notifications** [](cite:cites LDN) is another protocol developed by W3C. It shares similarities with earlier discussed technologies such as Kafka and MQTT. Instead of a Broker, it uses an Inbox. Senders make use of HTTP POST-requests which they send to the ‘Receiver’. The receiver is responsible for the Inbox, in which notifications will be stored. Consumers also consult the receiver, with GET-requests, on a GET-request to the Inbox URL, the receiver will return a listing of all notifications, each notification has a URI and must be an RDF source. The consumer can then request specific notifications from the Inbox. Each notification needs to have a JSON-LD content-type according to the specification, other serializations are optional. This clearly shows the similarities with a system that uses Brokers, however, LDN seems less focused on being lightweight and on continuous updating. Depending on the type of data, LDN might prove useful.


[](cite:cites van2020comparing) compares an HTTP polling approach with a push-based approach (using SSE or Server-Sent Events) on live Open datasets. The datasets used are real-time in nature and most of them are updated almost continuously. The paper clearly shows that, in order to minimize latency on the client, a push-based approach is required, where the server keeps a connection open to the client, so it can send multiple updates of the dataset. In the paper, **Server-sent-events** is used as an implementation of the push-based approach, also discussed are **WebSockets**, which uses an HTTP handshake but further transmission happens over a raw TCP connection and it supports bidirectional communication. SSE and WebSockets have similar performance and since SSE is unidirectional and only reliant on HTTP, the former was chosen in this article. Moreover, the CPU usage of SSE in comparison to polling was also shown to lower. For the purpose that we want to achieve [how to phrase this?] which is achieving a generic approach, for all kinds of Linked Open Datasets, it is clear that some sort of pushing from the server should be supported, since otherwise, the technology will not offer good support for fast-changing, continuously updating datasets. 


**WebSub** [](cite:cites WebSub) is another protocol that provides publish-subscribe communication using HTTP. Previously known as PubSubHubbub, it was adopted by W3C in 2018. The protocol uses ‘Hubs’ as intermediary servers between the publishers, who own topics and update them, and subscribers, who subscribe to topics. Subscribers need to be network-accessible at all times using their "Subscriber Callback URL". Subscribers subscribe to a topic using HTTP POST to a hub and have to specify their callback URL. To publish a data update, publishers have to inform and update the hubs in some way, the protocol does not specify a mechanism, hence it can be chosen by the users. Hubs will then update the subscribers in realtime by sending updates to the specified callback URL. The specification recommends using HTTPS for all performed requests, it also specifies an extra security header namely "X-Hub-Signature" which can be used for extra authentication. This protocol might be of interest since it inherently supports decentralization, namely, hubs can be partitioned according to the needs of the user and so can the topic, i.e. topics could be duplicated across different hubs or could only be present on certain hubs. The downside to this protocol is that it requires the "clients" (i.e. the users of the data) to always be available over HTTP, so they would have to run a server.
