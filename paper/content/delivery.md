## Data delivery
{:#delivery}

In this section, the transmission of the data to the users is discussed. Since storing data was previously discussed, the next problem that arises is how to expose it for consumers to access. The most commonly used protocol for web data exchange is HTTP, which uses a `request-response` communication pattern. However, this might not be the most optimal form of communication for the purpose of this paper. The MQTT protocol, which uses a `publish-subscribe` pattern or existing software such as Apache Kafka, which employs a similar strategy and uses its own protocol directly over TCP, can offer a more suitable pattern. Since in this case, updates are pushed automatically and do not need to be requested by the users. However, MQTT is a protocol which is not built upon HTTP and it is not standardized by the W3C Web of Things (instead it is more focused on IoT, Internet of Things, which concerns only the "things", the devices), therefore the protocol itself is not useful in this context, since we will use the web, but the characteristics of the protocol are useful. Alternatively, regular HTTP can still be viable, however, as will be discussed, a new kind of 'protocol' will have to be built on top of it, or an already existing protocol with the right characteristics, to better meet the needs of event-based processing. 

### Delivery Approaches

When delivering datasets, and more specifically, Linked Open Datasets, some considerations must be taken into account. The partitioning of the data impacts the transmission of it. Since the goal is to work with event streams, the architecture should be decentralized. Also, this approach should provide both storage and performance benefits in comparison with a centralized, data-dump style approach [](cite:cites delva2020geospatial).

In the current state of the art, the nature of the data affects the transmission. In the case of real-time sensor data, for example, [Atmoko et al.](cite:cites atmoko2017) describe an MQTT-based approach and indicate that this is more efficient than using regular HTTP. However, since the goal is to achieve a generic approach for each form of data, elements of different technologies will need to be combined. On one hand, machines are able to consume the delivered data, but a human user should also be able to check out the data in the browser.

#### Low-level protocols

Various low-level protocols exist such as Message Queuing Telemetry Transport (MQTT), the Advanced Message Queuing Protocol (AMQP), or the Constrained Application Protocol (CoAP). These protocols are focused on the transmission of data on constrained, low-level devices. [Wang] shows a similar conclusion when comparing MQTT and HTTP on data from IoT devices. Some datasets used by [Van de Vyvere et al.](cite:cites van2020comparing) also consist of IoT data. While, in this context, the intention is not to use a lightweight, lower level protocol such as MQTT (instead the goal is to publish data on the web, using HTTP), it is valuable to look at what makes it the best option, i.e. the different communication model, and adapt the desired technology so it incorporates this.

Furthermore, low-level protocols like MQTT suffer from security vulnerabilities [^cve], which is problematic in the context of open data publishing. HTTP, on the other hand, is extremely established, mainstream and way less susceptible to the same type of problems. Hence choosing HTTP is a necessity in terms of security.

[^cve]: https://www.xml.com/pub/a/2002/12/18/dive-into-xml.html

#### HTTP-based approaches
Other options include the use of **web feeds**, such as RDF Site Summary (RSS). RSS publishes updates in a feed and allows users to access them in a standardized format [^rssspec]. However, RSS is an umbrella term that spans different formats. Therefore, [Atom](cite:cites gregorio2005atom,ruby2008rss) was created, to achieve more standardization and disambiguation. It uses a separate protocol on top of HTTP. These two similar approaches to data publishing can both be considered, since the concept and use of web feeds is similar to what the event-based approach wants to achieve. 

[^rssspec]: https://www.xml.com/pub/a/2002/12/18/dive-into-xml.html

**Linked Data Notifications** [](cite:cites LDN) [](cite:cites capadisli2017linked) (LDN) is another protocol developed by the W3C. It shares similarities with earlier discussed technologies such as Kafka and MQTT. Instead of a Broker, which is used by Apache Kafka to serve as an intermediary, it uses an Inbox, which serves as an intermediary between the Senders and Consumers. 
<figure id="LDN">
<img src="images/LDN.svg" alt="[LDN model]">
<figcaption markdown="block">
Linked Data Notifications delivery model [](cite:cites capadisli2017linked). Note the different HTTP requests indictated using the arrows, as discussed in the text. Also note the discovery of the inbox and notifications using RDF predicates.
</figcaption>
</figure>
Senders make use of HTTP POST-requests which they send to the ‘Receiver’. The receiver is responsible for the Inbox, in which notifications are stored. Consumers also consult the receiver, with GET-requests, as their goal is to consume the data. As a response on the GET-request to the Inbox URL, the receiver returns a listing of all notifications. Each notification has a URI and must be an RDF source. The consumer can then request specific notifications from the Inbox. Each notification needs to have a JSON-LD content-type according to the specification, but other serializations are optional. This clearly shows the similarities with a system that uses Brokers. However, LDN seems less focused on continuously updating data and speed and more so on adaptability and the ability to use it in various different contexts as mentioned it [](cite:cites capadisli2017linked). Evaluating the performance of LDN, when speed is crucial, can be a topic for further research.

[Van de Vyvere et al.](cite:cites van2020comparing) compares an HTTP polling approach with a push-based approach (using SSE or Server-Sent Events) on live Open datasets. The datasets used are real-time in nature and most of them are updated almost continuously. The authors clearly show that in order to minimize latency on the client, a push-based approach is required, where the server keeps a connection open to the client, so it can send multiple updates of the dataset. In this paper, **Server-sent-events** is used as an implementation of the push-based approach. It also discusses **WebSockets**, which uses an HTTP handshake but further transmission happens over a raw TCP connection and it supports bidirectional communication. SSE and WebSockets have similar performance and since SSE is unidirectional and only reliant on HTTP, the former was chosen in this article. Moreover, the CPU usage of SSE in comparison to polling was also shown to decrease. To achieve the goal, i.e. a generic approach for all kinds of Linked Open datasets, it is clear that some sort of pushing from the server should be supported, since otherwise, the technology does not offer good support for fast-changing, continuously updating datasets. 


**WebSub** [](cite:cites WebSub) is another protocol that provides publish-subscribe communication using HTTP. Previously known as PubSubHubbub, it was adopted by the W3C in 2018. The protocol uses ‘Hubs’ as intermediary servers between the publishers, who own topics and update them, and subscribers, who subscribe to topics. Subscribers need to be network-accessible at all times using their "Subscriber Callback URL". Subscribers subscribe to a topic using HTTP POST to a hub and have to specify their callback URL. To publish a data update, publishers have to inform and update the hubs in some way, the protocol does not specify a mechanism, hence it can be chosen by the users. Hubs then update the subscribers in realtime by sending updates to the specified callback URL. The specification recommends using HTTPS for all performed requests, it also specifies an extra security header namely "X-Hub-Signature" <span class="comment" data-author="HD">in the latex version, put these in a texttt environment</span> which can be used for extra authentication. This protocol might be of interest since it inherently supports decentralization, namely, hubs can be partitioned according to the needs of the user and so can the topic (i.e. topics can be duplicated across different hubs or could only be present on certain hubs). The downside to this protocol is that it requires the "clients" (i.e. the users of the data) to always be available over HTTP, so a server must be available at the client at all times, to which updates can be pushed. 