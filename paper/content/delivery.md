## Data delivery
{:#delivery}

In this chapter, the transmission of the data to the users is discussed. Since storing data was previously discussed, the next problem that arises is how to expose it for consumers to access. The current most commonly used protocol for web data exchange is HTTP, which uses a `request-response` communication pattern. However, this might not be the most optimal form of communication for the purpose of this paper. The MQTT protocol, which uses a `publish-subscribe` pattern or existing software such as Apache Kafka, which employs a similar strategy and uses its own protocol directly over TCP, might be more suitable. Alternatively, regular HTTP can still be viable, however, a new kind of 'protocol' will have to be built on top of it, to better meet the needs of event-based processing.

### Considerations
When delivering datasets, and more specifically, Linked Open Datasets, some considerations must be taken into account.  

The partitioning of the data will impact the transmission of it. Since the goal is to work with event streams, the architecture will be decentralized. Also, this approach should provide both storage and performance benefits in comparison with a centralized, data-dump style approach [](cite:cites delva2020geospatial).

In the current state of the art, the nature of the data impacts the transmission. In the case of real-time sensor data, for example, [Atmoko et al.](cite:cites atmoko2017) describe an MQTT-based approach and indicate that this is more efficient than using regular HTTP. However, since the goal is to achieve a generic approach for each form of data, elements of different technologies will need to be combined.

### Delivery Approaches

#### Low-level protocols

Various low-level protocols exist such as Message Queuing Telemetry Transport (MQTT), the Advanced Message Queuing Protocol (AMQP), or the Constrained Application Protocol (CoAP). These protocols are focused on the transmission of data on constrained, low-level devices. [Wang] shows a similar conclusion when comparing MQTT and HTTP on data from IoT devices. Some datasets used by [Van de Vyvere et al.](cite:cites van2020comparing) will also consist of IoT data. While, in this context, the intention is not to use a lightweight, lower level protocol such as MQTT (instead the goal is to publish data on the web, using HTTP), it is valuable to look at what makes it the best option, i.e. the different communication model, and adapt the desired technology so it incorporates this.

Furthermore, low-level protocols like MQTT suffer from security vulnerabilities [](cite:cites cve), which is problematic in the context of open data publishing. HTTP, on the other hand, is extremely established, mainstream and way less susceptible to the same type of problems. Hence choosing HTTP is a necessity in terms of security.

#### HTTP-based approaches
Other options include the utilization of **web feeds**, such as (most commonly) RDF Site Summary (RSS). RSS [](cite:cites rsspilgrim) publishes updates in a feed and allows users to access them in a standardized format. However, RSS is an umbrella term that spans different formats. Therefore, [Atom](cite:cites gregorio2005atom,ruby2008rss) was created, to achieve more standardization and disambiguation. It uses a separate protocol on top of HTTP. These two approaches to data publishing (which are similar) should both be considered, since the concept and use of web feeds is similar to what the event-based approach wants to achieve. However, it is hard to determine a clear ''best option'' without further research (i.e. experimenting with these techniques).


**Linked Data Notifications** [](cite:cites LDN) is another protocol developed by the W3C. It shares similarities with earlier discussed technologies such as Kafka and MQTT. Instead of a Broker, which is what Kafka uses, it uses an Inbox. Senders make use of HTTP POST-requests which they send to the ‘Receiver’. The receiver is responsible for the Inbox, in which notifications will be stored. Consumers also consult the receiver, with GET-requests, as their goal is to consume the data. As a response on the GET-request to the Inbox URL, the receiver will return a listing of all notifications. Each notification has a URI and must be an RDF source. The consumer can then request specific notifications from the Inbox. Each notification needs to have a JSON-LD content-type according to the specification, but other serializations are optional. This clearly shows the similarities with a system that uses Brokers. However, LDN seems less focused on being lightweight and on continuous updating. Depending on the type of data, LDN might prove useful.


[Van de Vyvere et al.](cite:cites van2020comparing) compares an HTTP polling approach with a push-based approach (using SSE or Server-Sent Events) on live Open datasets. The datasets used are real-time in nature and most of them are updated almost continuously. The authors clearly show that in order to minimize latency on the client, a push-based approach is required, where the server keeps a connection open to the client, so it can send multiple updates of the dataset. In this paper, **Server-sent-events** is used as an implementation of the push-based approach. It also discusses **WebSockets**, which uses an HTTP handshake but further transmission happens over a raw TCP connection and it supports bidirectional communication. SSE and WebSockets have similar performance and since SSE is unidirectional and only reliant on HTTP, the former was chosen in this article. Moreover, the CPU usage of SSE in comparison to polling was also shown to decrease. To achieve the goal, i.e. a generic approach for all kinds of Linked Open Datasets, it is clear that some sort of pushing from the server should be supported, since otherwise, the technology will not offer good support for fast-changing, continuously updating datasets. 


**WebSub** [](cite:cites WebSub) is another protocol that provides publish-subscribe communication using HTTP. Previously known as PubSubHubbub, it was adopted by the W3C in 2018. The protocol uses ‘Hubs’ as intermediary servers between the publishers, who own topics and update them, and subscribers, who subscribe to topics. Subscribers need to be network-accessible at all times using their "Subscriber Callback URL". Subscribers subscribe to a topic using HTTP POST to a hub and have to specify their callback URL. To publish a data update, publishers have to inform and update the hubs in some way, the protocol does not specify a mechanism, hence it can be chosen by the users. Hubs will then update the subscribers in realtime by sending updates to the specified callback URL. The specification recommends using HTTPS for all performed requests, it also specifies an extra security header namely "X-Hub-Signature" which can be used for extra authentication. This protocol might be of interest since it inherently supports decentralization, namely, hubs can be partitioned according to the needs of the user and so can the topic (i.e. topics can be duplicated across different hubs or could only be present on certain hubs). The downside to this protocol is that it requires the "clients" (i.e. the users of the data) to always be available over HTTP, so a server must be available at all times, to which updates will be pushed.
